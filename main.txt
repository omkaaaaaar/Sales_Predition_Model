steps for final result product 

1.importing dependencies
2.Data Collection and Processing
(collect all internal steps from colab notebook)
3.Categorial Features 
4.Handling missing values
5.Mean --> average
  Mode --> more repeated value
6.Data Analysis 
7.Numerical Features
8.Categorical Features
9.Data Pre-Processing
10.Label Encoding
11.Splitting features and Target
12.Splitting the data into Training data & Testing Data
13.Machine Learning Model Training
14.Evaluation


imp libraries :

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics




sample code :

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load or define the model (this should ideally be trained and saved in a separate script)
def train_xgboost_model(data):
    # Data preprocessing: One-hot encoding, handling missing values, etc.
    data = pd.get_dummies(data, columns=['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 
                                         'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type'], drop_first=True)
    
    # Splitting features and target variable
    X = data.drop(columns=['Sales', 'Item_Identifier'])
    y = data['Sales']
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Define the XGBoost model
    model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6)
    
    # Train the model
    model.fit(X_train, y_train)
    
    # Evaluate the model
    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    print(f"Test RMSE: {rmse}")
    
    return model


# Predict using the trained model
def predict_sales(data):
    # Clean column names
    data.columns = data.columns.str.strip()
    
    # Required columns
    required_columns = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 
                        'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
    
    # Check for missing columns
    missing_columns = [col for col in required_columns if col not in data.columns]
    if missing_columns:
        raise KeyError(f"Missing columns in input data: {missing_columns}")

    # One-hot encoding karo
    data = pd.get_dummies(data, columns=required_columns, drop_first=True)
    
    # Ensure 'Sales' and 'Item_Identifier' are in the DataFrame
    if 'Sales' not in data.columns or 'Item_Identifier' not in data.columns:
        raise KeyError("Sales or Item_Identifier column is missing.")

    # Predict sales
    model = train_xgboost_model(data)

    # Make predictions
    predictions = model.predict(data.drop(columns=['Sales', 'Item_Identifier']))
    
    return predictions





"""
old code :

import pandas as pd

def predict_sales(data):
    # Ensure necessary columns are present and clean the data
    # We will assume you handle missing values or incorrect data types in data_preprocessing.py
    
    # Example: A simple model using 'Item_MRP' to predict 'Sales' (replace this with your actual ML model)
    if 'Item_MRP' in data.columns:
        # Dummy logic: Sales = Item_MRP * some factor (you can replace this with your ML model)
        predictions = data['Item_MRP'] * 1.2  # Replace this with actual ML prediction
    else:
        # If columns required for prediction are missing
        predictions = ["Insufficient data for prediction"] * len(data)
    
    
    return predictions
"""




train_model.py :

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import joblib

# Load your dataset
data = pd.read_csv('train.csv')

# Create a dictionary to hold label encoders for categorical features
label_encoders = {}

# Apply Label Encoding to categorical columns
# Add 'Other' to categories if it does not exist
for column in ['Item_Identifier', 'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']:
    le = LabelEncoder()
    data[column] = data[column].apply(lambda x: x if x in le.classes_ else 'Other')
    
    # Fit the LabelEncoder and include 'Other'
    le.fit(list(data[column]) + ['Other'])
    data[column] = le.transform(data[column])
    label_encoders[column] = le  # Save the label encoder for future use


# Define features (X) and target (y)
X = data[['Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_Type', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']]
y = data['Sales']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)
model.fit(X_train, y_train)

# Evaluate the model (optional)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Save the trained model to a file
joblib.dump(model, 'pickle_models/sales_prediction_model.pkl')
print("Model saved as sales_prediction_model.pkl")

# Save the label encoders to a file
joblib.dump(label_encoders, 'pickle_models/label_encoders.pkl')
print("Label encoders saved as label_encoders.pkl")




    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>



def about(request):
    return render(request,'about.html')

def contact(request):
    return render(request,'contact_us.html')






class ResetPasswordForm(forms.ModelForm):
    new_password1 = forms.CharField(widget=forms.PasswordInput, label="New Password")
    new_password2 = forms.CharField(widget=forms.PasswordInput, label="Confirm New Password")

    class Meta:
        model = ForgotCustomUser
        fields = ['email', 'username', 'phone']  # Include email, username, and phone fields

    def clean(self):
        cleaned_data = super().clean()
        new_password1 = cleaned_data.get("new_password1")
        new_password2 = cleaned_data.get("new_password2")

        if new_password1 and new_password2 and new_password1 != new_password2:
            raise forms.ValidationError("Passwords do not match")

        return cleaned_data

    def save(self, commit=True):
        """
        Override save method to handle password update and save other user details.
        """
        # Save the user instance without committing to the database yet
        user = super().save(commit=False)
        
        # Get new passwords from cleaned data
        new_password1 = self.cleaned_data.get("new_password1")
        new_password2 = self.cleaned_data.get("new_password2")

        # Use the model method to set and save the new password
        user.set_passwords(new_password1, new_password2)

        # Optionally commit the user instance to the database
        if commit:
            user.save()

        return user


admin name : nikhil1
password : Shift + sutar1979



vri cmds :
Set-ExecutionPolicy Unrestricted -Scope Process
jupytr 



def filter_predictions(request):
    filter_value = request.GET.get("filter_value", "").lower()

    # Load the grouped data from session
    grouped_data = request.session.get("grouped_data", [])

    # Apply filtering
    if filter_value:
        filtered_data = [
            row for row in grouped_data 
            if any(filter_value in str(value).lower() for value in row.values())
        ]
    else:
        filtered_data = grouped_data

    # Store filtered data in session for visualization use
    request.session["filtered_data"] = filtered_data

    print("Filtered Data:", filtered_data)  # Debugging: Check filtered data

    return render(request, "grouped_predictions.html", {"grouped_data": filtered_data})


def visualize_filtered_data(request):
    # Retrieve filtered data from session
    filtered_data = request.session.get("filtered_data", [])

    if not filtered_data:
        print("No filtered data found in session.")
        return JsonResponse({"error": "No filtered data available to visualize."})

    # Convert to DataFrame
    df = pd.DataFrame(filtered_data)

    if df.empty:
        print("DataFrame is empty after filtering.")
        return JsonResponse({"error": "No filtered data available to visualize."})

    print("Filtered Data for Visualization:", df.head())  # Debugging

    # Ensure required columns exist
    if "category" in df.columns and "Predicted_Sales" in df.columns:
        fig = go.Figure(
            data=[
                go.Bar(
                    x=df["category"],
                    y=df["Predicted_Sales"],
                    marker=dict(color="blue"),
                    name="Predicted Sales",
                )
            ]
        )
        fig.update_layout(title="Filtered Predicted Sales by Category", xaxis_title="Category", yaxis_title="Predicted Sales")
    else:
        return JsonResponse({"error": "Required columns are missing in the dataset."})

    # Convert Plotly figure to HTML
    chart_html = fig.to_html(full_html=False)

    return JsonResponse({"chart_html": chart_html})
